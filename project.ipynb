{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, Conv1D, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input, merge\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "\n",
    "train_copy = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing data\n",
    "================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load the 3 features from csv file which are MARGIN, SHAPE, TEXTURE\n",
    "def load_num_data(train):\n",
    "    \n",
    "    # get the id and label from the our training data\n",
    "    ID = train.pop('id')\n",
    "    label = train.pop('species')\n",
    "    \n",
    "    #labe our training data from 0~99 and standardize the data\n",
    "    y = LabelEncoder().fit(label).transform(label)    \n",
    "    X = StandardScaler().fit(train).transform(train)\n",
    "    \n",
    "\n",
    "    return ID, X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the image data from image files\n",
    "==============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using AbhijeetMulgund's image processing function\n",
    "###cited:https://www.kaggle.com/abhmul/keras-convnet-lb-0-0052-w-visualization\n",
    "\n",
    "def resize_img(img, max_dim=80):\n",
    "    \"\"\"\n",
    "    Resize the image to so the maximum side is of size max_dim\n",
    "    Returns a new image of the right size\n",
    "    \"\"\"\n",
    "    # Get the axis with the larger dimension\n",
    "    max_ax = max((0, 1), key=lambda i: img.size[i])\n",
    "    # Scale both axes so the image's largest dimension is max_dim\n",
    "    scale = max_dim / float(img.size[max_ax])\n",
    "    return img.resize((int(img.size[0] * scale), int(img.size[1] * scale)))\n",
    "    \n",
    "\n",
    "\n",
    "def load_image_data(ids, max_dim=80, center=True):\n",
    "    \"\"\"\n",
    "    Takes as input an array of image ids and loads the images as numpy\n",
    "    arrays with the images resized so the longest side is max-dim length.\n",
    "    If center is True, then will place the image in the center of\n",
    "    the output array, otherwise it will be placed at the top-left corner.\n",
    "    \"\"\"\n",
    "    # Initialize the output array\n",
    "    # NOTE: Theano users comment line below and\n",
    "    X = np.empty((len(ids), max_dim, max_dim, 1))\n",
    "    # X = np.empty((len(ids), 1, max_dim, max_dim)) # uncomment this\n",
    "    for i, idee in enumerate(ids):\n",
    "        # Turn the image into an array\n",
    "        x = resize_img(load_img(os.path.join('images', str(idee) + '.jpg'), grayscale=True), max_dim=max_dim)\n",
    "        x = img_to_array(x)\n",
    "        # Get the corners of the bounding box for the image\n",
    "        # NOTE: Theano users comment the two lines below and\n",
    "        length = x.shape[0]\n",
    "        width = x.shape[1]\n",
    "        # length = x.shape[1] # uncomment this\n",
    "        # width = x.shape[2] # uncomment this\n",
    "        if center:\n",
    "            h1 = int((max_dim - length) / 2)\n",
    "            h2 = h1 + length\n",
    "            w1 = int((max_dim - width) / 2)\n",
    "            w2 = w1 + width\n",
    "        else:\n",
    "            h1, w1 = 0, 0\n",
    "            h2, w2 = (length, width)\n",
    "        # Insert into image matrix\n",
    "        # NOTE: Theano users comment line below and\n",
    "        X[i, h1:h2, w1:w2, 0:1] = x\n",
    "        # X[i, 0:1, h1:h2, w1:w2] = x  # uncomment this\n",
    "    # Scale the array values so they are between 0 and 1\n",
    "    return np.around(X / 255.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get our image data and spilt the data to 90% of training data and 0.1 for valiation data\n",
    "============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hsing\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1639: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "ID, X_num_tr, y = load_num_data(train)\n",
    "   \n",
    "X_img_tr = load_image_data(ID)\n",
    "    \n",
    "sss = StratifiedShuffleSplit(train_size=0.9, random_state=23)\n",
    "tr_index, te_index= next(sss.split(X_num_tr, y))\n",
    "X_num_val, X_img_val, y_val = X_num_tr[te_index], X_img_tr[te_index], y[te_index]\n",
    "X_num_tr, X_img_tr, y_tr = X_num_tr[tr_index], X_img_tr[tr_index], y[tr_index]\n",
    "   \n",
    "\n",
    "y_tr = np_utils.to_categorical(y_tr, 99)\n",
    "y_val = np_utils.to_categorical(y_val, 99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build our CNN model with keras functional API\n",
    "======================================\n",
    "the reason we use function API not Sequential is that we have to different input here one is image data and another is 3 different attritube(MARGIN, SHAPE, TEXTURE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 99)\n",
      "(99, 99)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hsing\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(5, (3, 3), input_shape=(80, 80, 1..., padding=\"same\")`\n",
      "  \n",
      "c:\\users\\hsing\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (5, 5), padding=\"same\")`\n",
      "  del sys.path[0]\n",
      "c:\\users\\hsing\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:29: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.merge import concatenate\n",
    "\n",
    "input1 = Input(shape=(80, 80, 1))\n",
    "\n",
    "input2 = Input(shape=(192,))\n",
    "\n",
    "# create our first layer\n",
    "x = Conv2D(5, 3, 3, input_shape=(80, 80, 1), border_mode='same')(input1)\n",
    "x = (Activation('relu'))(x)\n",
    "x = (MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))(x)\n",
    "\n",
    "#  the second layer\n",
    "x = (Conv2D(32, 5, 5, border_mode='same'))(x)\n",
    "x = (Activation('relu'))(x)\n",
    "x = (MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "\n",
    "# Concatenate the output of our convnet for input2\n",
    "concatenated = concatenate([x, input2])\n",
    "\n",
    "x = Dense(101, activation='relu')(concatenated)\n",
    "x = Dropout(.4)(x)\n",
    "\n",
    "out = Dense(99, activation='softmax')(x)\n",
    "\n",
    "# create our models for two inputs\n",
    "model = Model(input=[input1, input2], output=out)\n",
    "\n",
    "\n",
    "\n",
    "print(y_tr.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiments and Evaluation\n",
    "========================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hsing\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 891 samples, validate on 99 samples\n",
      "Epoch 1/40\n",
      "891/891 [==============================] - 5s 6ms/step - loss: 3.5759 - acc: 0.2121 - val_loss: 1.2132 - val_acc: 0.7980\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.21325, saving model to leaf_model.h5\n",
      "Epoch 2/40\n",
      "891/891 [==============================] - 5s 5ms/step - loss: 0.8764 - acc: 0.7587 - val_loss: 0.2697 - val_acc: 0.9192\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.21325 to 0.26967, saving model to leaf_model.h5\n",
      "Epoch 3/40\n",
      "891/891 [==============================] - 5s 6ms/step - loss: 0.3127 - acc: 0.9181 - val_loss: 0.1300 - val_acc: 0.9798\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.26967 to 0.12999, saving model to leaf_model.h5\n",
      "Epoch 4/40\n",
      "891/891 [==============================] - 5s 6ms/step - loss: 0.1695 - acc: 0.9506 - val_loss: 0.1509 - val_acc: 0.9596\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.12999\n",
      "Epoch 5/40\n",
      "891/891 [==============================] - 6s 6ms/step - loss: 0.0818 - acc: 0.9787 - val_loss: 0.1117 - val_acc: 0.9798\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.12999 to 0.11174, saving model to leaf_model.h5\n",
      "Epoch 6/40\n",
      "891/891 [==============================] - 6s 6ms/step - loss: 0.0845 - acc: 0.9798 - val_loss: 0.1271 - val_acc: 0.9697\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.11174\n",
      "Epoch 7/40\n",
      "891/891 [==============================] - 5s 6ms/step - loss: 0.0634 - acc: 0.9843 - val_loss: 0.1276 - val_acc: 0.9495\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.11174\n",
      "Epoch 8/40\n",
      "891/891 [==============================] - 5s 6ms/step - loss: 0.0602 - acc: 0.9809 - val_loss: 0.1252 - val_acc: 0.9596\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.11174\n",
      "Epoch 9/40\n",
      "891/891 [==============================] - 5s 6ms/step - loss: 0.0545 - acc: 0.9865 - val_loss: 0.1986 - val_acc: 0.9596\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.11174\n",
      "Epoch 10/40\n",
      "891/891 [==============================] - 5s 6ms/step - loss: 0.0450 - acc: 0.9888 - val_loss: 0.0905 - val_acc: 0.9798\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.11174 to 0.09054, saving model to leaf_model.h5\n",
      "Epoch 11/40\n",
      "891/891 [==============================] - 5s 6ms/step - loss: 0.0403 - acc: 0.9888 - val_loss: 0.0826 - val_acc: 0.9798\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.09054 to 0.08263, saving model to leaf_model.h5\n",
      "Epoch 12/40\n",
      "891/891 [==============================] - 5s 6ms/step - loss: 0.0214 - acc: 0.9966 - val_loss: 0.1676 - val_acc: 0.9596\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.08263\n",
      "Epoch 13/40\n",
      "891/891 [==============================] - 5s 6ms/step - loss: 0.0234 - acc: 0.9910 - val_loss: 0.1432 - val_acc: 0.9596\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.08263\n",
      "Epoch 14/40\n",
      "891/891 [==============================] - 5s 6ms/step - loss: 0.0283 - acc: 0.9888 - val_loss: 0.1323 - val_acc: 0.9697\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.08263\n",
      "Epoch 15/40\n",
      "891/891 [==============================] - 6s 7ms/step - loss: 0.0433 - acc: 0.9854 - val_loss: 0.1259 - val_acc: 0.9798\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.08263\n",
      "Epoch 16/40\n",
      "891/891 [==============================] - 5s 6ms/step - loss: 0.0224 - acc: 0.9978 - val_loss: 0.1549 - val_acc: 0.9596\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.08263\n",
      "Epoch 17/40\n",
      "891/891 [==============================] - 6s 7ms/step - loss: 0.0213 - acc: 0.9921 - val_loss: 0.1086 - val_acc: 0.9596\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.08263\n",
      "Epoch 18/40\n",
      "891/891 [==============================] - 6s 7ms/step - loss: 0.0188 - acc: 0.9966 - val_loss: 0.1507 - val_acc: 0.9394\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.08263\n",
      "Epoch 19/40\n",
      "891/891 [==============================] - 6s 7ms/step - loss: 0.0299 - acc: 0.9921 - val_loss: 0.1293 - val_acc: 0.9394\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.08263\n",
      "Epoch 20/40\n",
      "891/891 [==============================] - 6s 7ms/step - loss: 0.0240 - acc: 0.9955 - val_loss: 0.1096 - val_acc: 0.9697\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.08263\n",
      "Epoch 21/40\n",
      "891/891 [==============================] - 6s 7ms/step - loss: 0.0668 - acc: 0.9776 - val_loss: 0.1257 - val_acc: 0.9596\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.08263\n",
      "Epoch 22/40\n",
      "891/891 [==============================] - 5s 6ms/step - loss: 0.0667 - acc: 0.9787 - val_loss: 0.1681 - val_acc: 0.9596\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.08263\n",
      "Epoch 23/40\n",
      "891/891 [==============================] - 6s 6ms/step - loss: 0.0397 - acc: 0.9888 - val_loss: 0.1435 - val_acc: 0.9596\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.08263\n",
      "Epoch 24/40\n",
      "891/891 [==============================] - 6s 6ms/step - loss: 0.0515 - acc: 0.9865 - val_loss: 0.1680 - val_acc: 0.9697\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.08263\n",
      "Epoch 25/40\n",
      "891/891 [==============================] - 5s 6ms/step - loss: 0.0228 - acc: 0.9944 - val_loss: 0.1329 - val_acc: 0.9596\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.08263\n",
      "Epoch 26/40\n",
      "891/891 [==============================] - 6s 7ms/step - loss: 0.0238 - acc: 0.9955 - val_loss: 0.1310 - val_acc: 0.9596\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.08263\n",
      "Epoch 27/40\n",
      "891/891 [==============================] - 6s 7ms/step - loss: 0.0438 - acc: 0.9832 - val_loss: 0.0244 - val_acc: 0.9899\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.08263 to 0.02443, saving model to leaf_model.h5\n",
      "Epoch 28/40\n",
      "891/891 [==============================] - 6s 7ms/step - loss: 0.0504 - acc: 0.9888 - val_loss: 0.0993 - val_acc: 0.9697\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.02443\n",
      "Epoch 29/40\n",
      "891/891 [==============================] - 6s 7ms/step - loss: 0.0587 - acc: 0.9843 - val_loss: 0.1776 - val_acc: 0.9293\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.02443\n",
      "Epoch 30/40\n",
      "891/891 [==============================] - 6s 7ms/step - loss: 0.0683 - acc: 0.9776 - val_loss: 0.2084 - val_acc: 0.9495\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.02443\n",
      "Epoch 31/40\n",
      "891/891 [==============================] - 6s 7ms/step - loss: 0.0769 - acc: 0.9787 - val_loss: 0.2354 - val_acc: 0.9495\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.02443\n",
      "Epoch 32/40\n",
      "891/891 [==============================] - 6s 7ms/step - loss: 0.0692 - acc: 0.9764 - val_loss: 0.2143 - val_acc: 0.9495\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.02443\n",
      "Epoch 33/40\n",
      "891/891 [==============================] - 7s 7ms/step - loss: 0.0653 - acc: 0.9809 - val_loss: 0.1554 - val_acc: 0.9596\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.02443\n",
      "Epoch 34/40\n",
      "891/891 [==============================] - 7s 8ms/step - loss: 0.0480 - acc: 0.9809 - val_loss: 0.3297 - val_acc: 0.9293\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.02443\n",
      "Epoch 35/40\n",
      "891/891 [==============================] - 6s 7ms/step - loss: 0.0635 - acc: 0.9854 - val_loss: 0.2216 - val_acc: 0.9596\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.02443\n",
      "Epoch 36/40\n",
      "891/891 [==============================] - 6s 7ms/step - loss: 0.0610 - acc: 0.9854 - val_loss: 0.3563 - val_acc: 0.9596\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.02443\n",
      "Epoch 37/40\n",
      "891/891 [==============================] - 6s 7ms/step - loss: 0.0683 - acc: 0.9832 - val_loss: 0.2707 - val_acc: 0.9596\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.02443\n",
      "Epoch 38/40\n",
      "891/891 [==============================] - 6s 7ms/step - loss: 0.0696 - acc: 0.9798 - val_loss: 0.2453 - val_acc: 0.9495\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.02443\n",
      "Epoch 39/40\n",
      "891/891 [==============================] - 5s 6ms/step - loss: 0.0437 - acc: 0.9865 - val_loss: 0.2430 - val_acc: 0.9596\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.02443\n",
      "Epoch 40/40\n",
      "891/891 [==============================] - 5s 6ms/step - loss: 0.0533 - acc: 0.9820 - val_loss: 0.1507 - val_acc: 0.9697\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.02443\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "#sgd = SGD(lr=0.001, decay=1e-4, momentum=0.9)\n",
    "#we have try out different kind of optimizers(Adam, sgd, rmsprop) and our best model is using the Adam\n",
    "optim = Adam(lr=0.005, beta_1=0.9,beta_2=0.999, epsilon= None, decay=0.0, amsgrad=True)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=optim,metrics=['accuracy'])\n",
    "\n",
    "# we store our best model in leaf_model.h5 files\n",
    "best_leaf_clafier = \"leaf_model.h5\"\n",
    "best_model = ModelCheckpoint(best_leaf_clafier, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "#here is the paramenter we find for the best score\n",
    "nb_epoch = 40\n",
    "batch_size=64\n",
    "\n",
    "model.fit([X_img_tr, X_num_tr], y_tr, nb_epoch=nb_epoch, validation_data=([X_img_val, X_num_val], y_val), batch_size=32, callbacks=[best_model])\n",
    "\n",
    "model = load_model(best_leaf_clafier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run on testing dataset\n",
    "=================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "803953"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "te_ID = test.pop('id')\n",
    "\n",
    "x_num_te = StandardScaler().fit(test).transform(test)\n",
    "\n",
    "x_img_te = load_image_data(te_ID)\n",
    "yPred = model.predict([x_img_te, x_num_te])\n",
    "yPred = pd.DataFrame(yPred,index=te_ID,columns=sorted(train_copy.species.unique()))\n",
    "fp = open('submission_nn_kernel1.csv','w')\n",
    "fp.write(yPred.to_csv())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
